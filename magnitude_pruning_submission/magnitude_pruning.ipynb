{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU05O4JFNJYP",
        "outputId": "8d36fab5-7681-44fb-9c69-88e564f1b788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_images.pkl\n",
            "train_labels.pkl\n",
            "val_images.pkl\n",
            "val_labels.pkl\n"
          ]
        }
      ],
      "source": [
        "# untar\n",
        "!tar -xvzf dataset.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rBvx8SJy8SO",
        "outputId": "696a219a-f0a4-4f5f-dd77-2d4bf6462118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models, regularizers\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(tf.version.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iddSEYi6y8SP"
      },
      "outputs": [],
      "source": [
        "root_dir = \"../pruned_models_and_notebooks_global_thresholding/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BKIu_Xuzy8SQ"
      },
      "outputs": [],
      "source": [
        "# load train\n",
        "train_images = pickle.load(open(\"../train_images.pkl\", \"rb\"))\n",
        "train_labels = pickle.load(open(\"../train_labels.pkl\", \"rb\"))\n",
        "\n",
        "# load val\n",
        "val_images = pickle.load(open(\"../val_images.pkl\", \"rb\"))\n",
        "val_labels = pickle.load(open(\"../val_labels.pkl\", \"rb\"))\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QOvE0odzy8SQ"
      },
      "outputs": [],
      "source": [
        "class CustomModel(keras.models.Sequential):\n",
        "    def __init__(self, weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.custom_masks = [tf.cast(weight != 0, tf.float32) for weight in weights]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        trainable_vars = self.trainable_variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        gradients = [tf.multiply(grad, self.custom_masks[i]) for i, grad in enumerate(gradients)]\n",
        "        \n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        \n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "    \n",
        "def get_custom_model(weights):\n",
        "    model = CustomModel(deepcopy(weights))\n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", kernel_regularizer=regularizers.l2(1e-5), input_shape=(25,25,3)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_regularizer=regularizers.l2(1e-5)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(5))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_original_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", kernel_regularizer=regularizers.l2(1e-5), input_shape=(25,25,3)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_regularizer=regularizers.l2(1e-5)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(1e-5)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(5))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KLCeIK_oy8SR"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_dataset = datagen.flow(train_images, train_labels, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K_f1jolFy8SR"
      },
      "outputs": [],
      "source": [
        "def measure_sparsity(weights):\n",
        "    num_zeros = 0\n",
        "    num_nonzeros = 0\n",
        "    for weight in weights:\n",
        "        z = tf.math.count_nonzero(tf.equal(weight, 0)).numpy()\n",
        "        nz = tf.size(weight).numpy() - z\n",
        "        num_zeros += z\n",
        "        num_nonzeros += nz\n",
        "\n",
        "    return num_zeros / (num_zeros + num_nonzeros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-_e89z40y8SR"
      },
      "outputs": [],
      "source": [
        "def categorical_loss_with_label_smoothing(y, yhat):\n",
        "    y = tf.one_hot(tf.cast(y, tf.int32), 5)\n",
        "    yhat = tf.expand_dims(yhat, axis=1)\n",
        "    return tf.keras.losses.categorical_crossentropy(y, yhat, label_smoothing=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mB4zQMiXy8SR"
      },
      "outputs": [],
      "source": [
        "def calc_std(weights):\n",
        "    tmp = []\n",
        "    for w in weights:\n",
        "        tmp.append(w.numpy().flatten())\n",
        "    oned_stackedweights = np.hstack(tmp)\n",
        "    return np.std(oned_stackedweights)\n",
        "\n",
        "\n",
        "def prune(weights, stdval, factor=0.1):\n",
        "    pruned_weights = deepcopy(weights)\n",
        "    threshold = stdval * factor\n",
        "    for i, w in enumerate(pruned_weights):\n",
        "        mask = tf.cast(tf.greater(\n",
        "            tf.abs(w), threshold), tf.float32)\n",
        "        pruned_weights[i] = (tf.multiply(w, mask))\n",
        "    return pruned_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iszpv_VKy8SR",
        "outputId": "666da55f-10d8-4555-f66e-6f8e963a3b07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 1.6245 - accuracy: 0.2364\n",
            "Epoch 1: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 15s 13ms/step - loss: 1.6245 - accuracy: 0.2364 - val_loss: 1.5781 - val_accuracy: 0.3251\n",
            "Epoch 2/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.5025 - accuracy: 0.4046\n",
            "Epoch 2: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.5023 - accuracy: 0.4049 - val_loss: 1.4419 - val_accuracy: 0.4836\n",
            "Epoch 3/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 1.3584 - accuracy: 0.5004\n",
            "Epoch 3: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.3570 - accuracy: 0.5008 - val_loss: 1.3002 - val_accuracy: 0.5390\n",
            "Epoch 4/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 1.2539 - accuracy: 0.5544\n",
            "Epoch 4: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.2523 - accuracy: 0.5552 - val_loss: 1.2167 - val_accuracy: 0.5818\n",
            "Epoch 5/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 1.1905 - accuracy: 0.5894\n",
            "Epoch 5: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.1897 - accuracy: 0.5902 - val_loss: 1.1654 - val_accuracy: 0.6139\n",
            "Epoch 6/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.1561 - accuracy: 0.6125\n",
            "Epoch 6: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.1558 - accuracy: 0.6127 - val_loss: 1.1345 - val_accuracy: 0.6261\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.1252 - accuracy: 0.6304\n",
            "Epoch 7: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.1249 - accuracy: 0.6306 - val_loss: 1.1090 - val_accuracy: 0.6364\n",
            "Epoch 8/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0947 - accuracy: 0.6481\n",
            "Epoch 8: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0948 - accuracy: 0.6482 - val_loss: 1.0909 - val_accuracy: 0.6566\n",
            "Epoch 9/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 1.0803 - accuracy: 0.6562\n",
            "Epoch 9: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0802 - accuracy: 0.6561 - val_loss: 1.0747 - val_accuracy: 0.6669\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0638 - accuracy: 0.6649\n",
            "Epoch 10: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0641 - accuracy: 0.6648 - val_loss: 1.0607 - val_accuracy: 0.6725\n",
            "Epoch 11/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0518 - accuracy: 0.6694\n",
            "Epoch 11: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0517 - accuracy: 0.6696 - val_loss: 1.0528 - val_accuracy: 0.6760\n",
            "Epoch 12/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0406 - accuracy: 0.6816\n",
            "Epoch 12: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.0410 - accuracy: 0.6812 - val_loss: 1.0447 - val_accuracy: 0.6816\n",
            "Epoch 13/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0295 - accuracy: 0.6820\n",
            "Epoch 13: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.0291 - accuracy: 0.6824 - val_loss: 1.0337 - val_accuracy: 0.6887\n",
            "Epoch 14/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0200 - accuracy: 0.6883\n",
            "Epoch 14: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.0199 - accuracy: 0.6885 - val_loss: 1.0277 - val_accuracy: 0.6867\n",
            "Epoch 15/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 1.0101 - accuracy: 0.6978\n",
            "Epoch 15: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0099 - accuracy: 0.6979 - val_loss: 1.0214 - val_accuracy: 0.6915\n",
            "Epoch 16/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0078 - accuracy: 0.6941\n",
            "Epoch 16: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0081 - accuracy: 0.6940 - val_loss: 1.0191 - val_accuracy: 0.6919\n",
            "Epoch 17/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 1.0038 - accuracy: 0.6975\n",
            "Epoch 17: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0037 - accuracy: 0.6978 - val_loss: 1.0153 - val_accuracy: 0.6907\n",
            "Epoch 18/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9977 - accuracy: 0.6993\n",
            "Epoch 18: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9976 - accuracy: 0.6995 - val_loss: 1.0095 - val_accuracy: 0.6998\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9900 - accuracy: 0.7043\n",
            "Epoch 19: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9896 - accuracy: 0.7046 - val_loss: 1.0041 - val_accuracy: 0.7022\n",
            "Epoch 20/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9848 - accuracy: 0.7096\n",
            "Epoch 20: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9849 - accuracy: 0.7097 - val_loss: 1.0027 - val_accuracy: 0.6994\n",
            "Epoch 21/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9827 - accuracy: 0.7086\n",
            "Epoch 21: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9828 - accuracy: 0.7081 - val_loss: 1.0009 - val_accuracy: 0.6958\n",
            "Epoch 22/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9780 - accuracy: 0.7122\n",
            "Epoch 22: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9780 - accuracy: 0.7122 - val_loss: 0.9963 - val_accuracy: 0.7065\n",
            "Epoch 23/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9743 - accuracy: 0.7104\n",
            "Epoch 23: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9743 - accuracy: 0.7104 - val_loss: 0.9952 - val_accuracy: 0.7069\n",
            "Epoch 24/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9705 - accuracy: 0.7150\n",
            "Epoch 24: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9704 - accuracy: 0.7149 - val_loss: 0.9915 - val_accuracy: 0.7038\n",
            "Epoch 25/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9735 - accuracy: 0.7153\n",
            "Epoch 25: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9734 - accuracy: 0.7153 - val_loss: 0.9890 - val_accuracy: 0.7097\n",
            "Epoch 26/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9651 - accuracy: 0.7199\n",
            "Epoch 26: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9651 - accuracy: 0.7198 - val_loss: 0.9889 - val_accuracy: 0.7073\n",
            "Epoch 27/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9605 - accuracy: 0.7214\n",
            "Epoch 27: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9612 - accuracy: 0.7207 - val_loss: 0.9862 - val_accuracy: 0.7117\n",
            "Epoch 28/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9611 - accuracy: 0.7209\n",
            "Epoch 28: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9610 - accuracy: 0.7211 - val_loss: 0.9883 - val_accuracy: 0.7097\n",
            "Epoch 29/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9576 - accuracy: 0.7257\n",
            "Epoch 29: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9575 - accuracy: 0.7259 - val_loss: 0.9826 - val_accuracy: 0.7117\n",
            "Epoch 30/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9567 - accuracy: 0.7217\n",
            "Epoch 30: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9570 - accuracy: 0.7216 - val_loss: 0.9836 - val_accuracy: 0.7097\n",
            "Epoch 31/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9514 - accuracy: 0.7292\n",
            "Epoch 31: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9518 - accuracy: 0.7290 - val_loss: 0.9796 - val_accuracy: 0.7149\n",
            "Epoch 32/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9546 - accuracy: 0.7282\n",
            "Epoch 32: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9547 - accuracy: 0.7279 - val_loss: 0.9794 - val_accuracy: 0.7133\n",
            "Epoch 33/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9519 - accuracy: 0.7280\n",
            "Epoch 33: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9515 - accuracy: 0.7281 - val_loss: 0.9755 - val_accuracy: 0.7141\n",
            "Epoch 34/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9498 - accuracy: 0.7268\n",
            "Epoch 34: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9490 - accuracy: 0.7271 - val_loss: 0.9765 - val_accuracy: 0.7141\n",
            "Epoch 35/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9478 - accuracy: 0.7267\n",
            "Epoch 35: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9475 - accuracy: 0.7272 - val_loss: 0.9736 - val_accuracy: 0.7145\n",
            "Epoch 36/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9429 - accuracy: 0.7337\n",
            "Epoch 36: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9430 - accuracy: 0.7337 - val_loss: 0.9736 - val_accuracy: 0.7141\n",
            "Epoch 37/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9359 - accuracy: 0.7383\n",
            "Epoch 37: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9366 - accuracy: 0.7380 - val_loss: 0.9724 - val_accuracy: 0.7145\n",
            "Epoch 38/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9381 - accuracy: 0.7351\n",
            "Epoch 38: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9376 - accuracy: 0.7354 - val_loss: 0.9737 - val_accuracy: 0.7141\n",
            "Epoch 39/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9354 - accuracy: 0.7359\n",
            "Epoch 39: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9358 - accuracy: 0.7360 - val_loss: 0.9683 - val_accuracy: 0.7168\n",
            "Epoch 40/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9300 - accuracy: 0.7382\n",
            "Epoch 40: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9299 - accuracy: 0.7383 - val_loss: 0.9694 - val_accuracy: 0.7160\n",
            "Epoch 41/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9341 - accuracy: 0.7374\n",
            "Epoch 41: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9342 - accuracy: 0.7372 - val_loss: 0.9678 - val_accuracy: 0.7184\n",
            "Epoch 42/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9314 - accuracy: 0.7375\n",
            "Epoch 42: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9312 - accuracy: 0.7378 - val_loss: 0.9652 - val_accuracy: 0.7180\n",
            "Epoch 43/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9298 - accuracy: 0.7404\n",
            "Epoch 43: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9300 - accuracy: 0.7397 - val_loss: 0.9653 - val_accuracy: 0.7152\n",
            "Epoch 44/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9288 - accuracy: 0.7399\n",
            "Epoch 44: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9290 - accuracy: 0.7398 - val_loss: 0.9629 - val_accuracy: 0.7188\n",
            "Epoch 45/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9265 - accuracy: 0.7386\n",
            "Epoch 45: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9264 - accuracy: 0.7385 - val_loss: 0.9623 - val_accuracy: 0.7196\n",
            "Epoch 46/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9220 - accuracy: 0.7416\n",
            "Epoch 46: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9217 - accuracy: 0.7418 - val_loss: 0.9619 - val_accuracy: 0.7192\n",
            "Epoch 47/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9226 - accuracy: 0.7436\n",
            "Epoch 47: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9228 - accuracy: 0.7434 - val_loss: 0.9587 - val_accuracy: 0.7192\n",
            "Epoch 48/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9222 - accuracy: 0.7441\n",
            "Epoch 48: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9221 - accuracy: 0.7442 - val_loss: 0.9626 - val_accuracy: 0.7184\n",
            "Epoch 49/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9221 - accuracy: 0.7473\n",
            "Epoch 49: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9216 - accuracy: 0.7478 - val_loss: 0.9589 - val_accuracy: 0.7196\n",
            "Epoch 50/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9228 - accuracy: 0.7436\n",
            "Epoch 50: saving model to magnitude_pruning_itr_0.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9228 - accuracy: 0.7441 - val_loss: 0.9627 - val_accuracy: 0.7212\n",
            "79/79 [==============================] - 0s 4ms/step - loss: 0.9627 - accuracy: 0.7212\n",
            "Post retraining val loss: 0.9627325534820557 | val acc: 0.7211881279945374 | sparsity: 0.9168725640165077\n",
            "Epoch 1/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 4.1351 - accuracy: 0.2926\n",
            "Epoch 1: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 4s 11ms/step - loss: 4.1309 - accuracy: 0.2926 - val_loss: 1.6206 - val_accuracy: 0.4186\n",
            "Epoch 2/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 2.0943 - accuracy: 0.3587\n",
            "Epoch 2: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 2.0916 - accuracy: 0.3587 - val_loss: 1.3458 - val_accuracy: 0.4939\n",
            "Epoch 3/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.5692 - accuracy: 0.4128\n",
            "Epoch 3: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 1.5683 - accuracy: 0.4129 - val_loss: 1.2936 - val_accuracy: 0.5192\n",
            "Epoch 4/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 1.4488 - accuracy: 0.4458\n",
            "Epoch 4: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.4481 - accuracy: 0.4460 - val_loss: 1.2655 - val_accuracy: 0.5461\n",
            "Epoch 5/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 1.3853 - accuracy: 0.4801\n",
            "Epoch 5: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.3853 - accuracy: 0.4798 - val_loss: 1.2310 - val_accuracy: 0.5683\n",
            "Epoch 6/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.3480 - accuracy: 0.4992\n",
            "Epoch 6: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.3476 - accuracy: 0.4995 - val_loss: 1.2052 - val_accuracy: 0.5818\n",
            "Epoch 7/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 1.2999 - accuracy: 0.5273\n",
            "Epoch 7: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.3001 - accuracy: 0.5271 - val_loss: 1.1703 - val_accuracy: 0.5988\n",
            "Epoch 8/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.2581 - accuracy: 0.5531\n",
            "Epoch 8: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.2581 - accuracy: 0.5531 - val_loss: 1.1447 - val_accuracy: 0.6087\n",
            "Epoch 9/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.2258 - accuracy: 0.5648\n",
            "Epoch 9: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.2257 - accuracy: 0.5650 - val_loss: 1.1191 - val_accuracy: 0.6250\n",
            "Epoch 10/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 1.1920 - accuracy: 0.5858\n",
            "Epoch 10: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.1921 - accuracy: 0.5853 - val_loss: 1.0947 - val_accuracy: 0.6388\n",
            "Epoch 11/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 1.1678 - accuracy: 0.6006\n",
            "Epoch 11: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.1679 - accuracy: 0.6006 - val_loss: 1.0756 - val_accuracy: 0.6467\n",
            "Epoch 12/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 1.1467 - accuracy: 0.6108\n",
            "Epoch 12: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.1474 - accuracy: 0.6105 - val_loss: 1.0598 - val_accuracy: 0.6515\n",
            "Epoch 13/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.1281 - accuracy: 0.6248\n",
            "Epoch 13: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.1281 - accuracy: 0.6247 - val_loss: 1.0488 - val_accuracy: 0.6574\n",
            "Epoch 14/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.1037 - accuracy: 0.6371\n",
            "Epoch 14: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.1037 - accuracy: 0.6372 - val_loss: 1.0388 - val_accuracy: 0.6594\n",
            "Epoch 15/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0918 - accuracy: 0.6441\n",
            "Epoch 15: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0918 - accuracy: 0.6440 - val_loss: 1.0285 - val_accuracy: 0.6745\n",
            "Epoch 16/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0812 - accuracy: 0.6452\n",
            "Epoch 16: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0811 - accuracy: 0.6453 - val_loss: 1.0227 - val_accuracy: 0.6709\n",
            "Epoch 17/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0657 - accuracy: 0.6569\n",
            "Epoch 17: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0663 - accuracy: 0.6564 - val_loss: 1.0175 - val_accuracy: 0.6737\n",
            "Epoch 18/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 1.0563 - accuracy: 0.6590\n",
            "Epoch 18: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0562 - accuracy: 0.6591 - val_loss: 1.0133 - val_accuracy: 0.6816\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0464 - accuracy: 0.6671\n",
            "Epoch 19: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0464 - accuracy: 0.6671 - val_loss: 1.0119 - val_accuracy: 0.6792\n",
            "Epoch 20/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 1.0454 - accuracy: 0.6662\n",
            "Epoch 20: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0459 - accuracy: 0.6661 - val_loss: 1.0092 - val_accuracy: 0.6812\n",
            "Epoch 21/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.6728\n",
            "Epoch 21: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0348 - accuracy: 0.6727 - val_loss: 1.0061 - val_accuracy: 0.6844\n",
            "Epoch 22/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 1.0291 - accuracy: 0.6772\n",
            "Epoch 22: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0299 - accuracy: 0.6765 - val_loss: 1.0011 - val_accuracy: 0.6879\n",
            "Epoch 23/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 1.0268 - accuracy: 0.6749\n",
            "Epoch 23: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0273 - accuracy: 0.6748 - val_loss: 0.9975 - val_accuracy: 0.6867\n",
            "Epoch 24/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 1.0240 - accuracy: 0.6760\n",
            "Epoch 24: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0240 - accuracy: 0.6760 - val_loss: 0.9997 - val_accuracy: 0.6859\n",
            "Epoch 25/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 1.0207 - accuracy: 0.6795\n",
            "Epoch 25: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0210 - accuracy: 0.6790 - val_loss: 0.9962 - val_accuracy: 0.6871\n",
            "Epoch 26/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0184 - accuracy: 0.6826\n",
            "Epoch 26: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0186 - accuracy: 0.6828 - val_loss: 0.9915 - val_accuracy: 0.6923\n",
            "Epoch 27/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0163 - accuracy: 0.6867\n",
            "Epoch 27: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0165 - accuracy: 0.6868 - val_loss: 0.9891 - val_accuracy: 0.6891\n",
            "Epoch 28/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0108 - accuracy: 0.6890\n",
            "Epoch 28: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0109 - accuracy: 0.6888 - val_loss: 0.9873 - val_accuracy: 0.6903\n",
            "Epoch 29/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0085 - accuracy: 0.6884\n",
            "Epoch 29: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0083 - accuracy: 0.6887 - val_loss: 0.9873 - val_accuracy: 0.6982\n",
            "Epoch 30/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0010 - accuracy: 0.6939\n",
            "Epoch 30: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0012 - accuracy: 0.6938 - val_loss: 0.9871 - val_accuracy: 0.6919\n",
            "Epoch 31/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 1.0053 - accuracy: 0.6895\n",
            "Epoch 31: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0053 - accuracy: 0.6893 - val_loss: 0.9856 - val_accuracy: 0.6947\n",
            "Epoch 32/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9985 - accuracy: 0.6941\n",
            "Epoch 32: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9982 - accuracy: 0.6944 - val_loss: 0.9839 - val_accuracy: 0.6927\n",
            "Epoch 33/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0005 - accuracy: 0.6920\n",
            "Epoch 33: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0007 - accuracy: 0.6918 - val_loss: 0.9830 - val_accuracy: 0.6895\n",
            "Epoch 34/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9966 - accuracy: 0.6960\n",
            "Epoch 34: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9964 - accuracy: 0.6962 - val_loss: 0.9825 - val_accuracy: 0.6887\n",
            "Epoch 35/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9937 - accuracy: 0.6992\n",
            "Epoch 35: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9939 - accuracy: 0.6990 - val_loss: 0.9784 - val_accuracy: 0.6966\n",
            "Epoch 36/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9929 - accuracy: 0.6971\n",
            "Epoch 36: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9930 - accuracy: 0.6971 - val_loss: 0.9796 - val_accuracy: 0.6899\n",
            "Epoch 37/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9903 - accuracy: 0.7002\n",
            "Epoch 37: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9901 - accuracy: 0.7003 - val_loss: 0.9767 - val_accuracy: 0.7018\n",
            "Epoch 38/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9877 - accuracy: 0.6999\n",
            "Epoch 38: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9874 - accuracy: 0.6998 - val_loss: 0.9775 - val_accuracy: 0.6950\n",
            "Epoch 39/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9861 - accuracy: 0.7037\n",
            "Epoch 39: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9862 - accuracy: 0.7034 - val_loss: 0.9740 - val_accuracy: 0.6990\n",
            "Epoch 40/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9869 - accuracy: 0.7016\n",
            "Epoch 40: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9874 - accuracy: 0.7013 - val_loss: 0.9772 - val_accuracy: 0.6998\n",
            "Epoch 41/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9904 - accuracy: 0.7001\n",
            "Epoch 41: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9903 - accuracy: 0.7001 - val_loss: 0.9772 - val_accuracy: 0.7018\n",
            "Epoch 42/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9864 - accuracy: 0.7033\n",
            "Epoch 42: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9868 - accuracy: 0.7031 - val_loss: 0.9728 - val_accuracy: 0.7002\n",
            "Epoch 43/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9852 - accuracy: 0.7014\n",
            "Epoch 43: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9841 - accuracy: 0.7015 - val_loss: 0.9728 - val_accuracy: 0.7046\n",
            "Epoch 44/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9796 - accuracy: 0.7058\n",
            "Epoch 44: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9791 - accuracy: 0.7061 - val_loss: 0.9715 - val_accuracy: 0.7010\n",
            "Epoch 45/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9815 - accuracy: 0.7054\n",
            "Epoch 45: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9807 - accuracy: 0.7060 - val_loss: 0.9700 - val_accuracy: 0.7038\n",
            "Epoch 46/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9777 - accuracy: 0.7059\n",
            "Epoch 46: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9780 - accuracy: 0.7057 - val_loss: 0.9689 - val_accuracy: 0.7002\n",
            "Epoch 47/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9779 - accuracy: 0.7051\n",
            "Epoch 47: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9774 - accuracy: 0.7053 - val_loss: 0.9689 - val_accuracy: 0.7069\n",
            "Epoch 48/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9745 - accuracy: 0.7076\n",
            "Epoch 48: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9746 - accuracy: 0.7075 - val_loss: 0.9673 - val_accuracy: 0.7105\n",
            "Epoch 49/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9747 - accuracy: 0.7080\n",
            "Epoch 49: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9755 - accuracy: 0.7077 - val_loss: 0.9671 - val_accuracy: 0.7077\n",
            "Epoch 50/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9791 - accuracy: 0.7078\n",
            "Epoch 50: saving model to magnitude_pruning_itr_1.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9788 - accuracy: 0.7080 - val_loss: 0.9670 - val_accuracy: 0.7077\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9670 - accuracy: 0.7077\n",
            "Post retraining val loss: 0.9669505953788757 | val acc: 0.7077227830886841 | sparsity: 0.9580610288177585\n",
            "Epoch 1/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 1.5970 - accuracy: 0.3973\n",
            "Epoch 1: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 5s 12ms/step - loss: 1.5895 - accuracy: 0.4004 - val_loss: 1.1999 - val_accuracy: 0.6166\n",
            "Epoch 2/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.1308 - accuracy: 0.6233\n",
            "Epoch 2: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.1310 - accuracy: 0.6233 - val_loss: 1.0319 - val_accuracy: 0.6780\n",
            "Epoch 3/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 1.0391 - accuracy: 0.6698\n",
            "Epoch 3: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0393 - accuracy: 0.6700 - val_loss: 0.9995 - val_accuracy: 0.6871\n",
            "Epoch 4/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 1.0165 - accuracy: 0.6831\n",
            "Epoch 4: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0166 - accuracy: 0.6832 - val_loss: 0.9880 - val_accuracy: 0.6978\n",
            "Epoch 5/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 1.0031 - accuracy: 0.6895\n",
            "Epoch 5: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 1.0029 - accuracy: 0.6898 - val_loss: 0.9812 - val_accuracy: 0.6962\n",
            "Epoch 6/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9975 - accuracy: 0.6953\n",
            "Epoch 6: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9978 - accuracy: 0.6950 - val_loss: 0.9768 - val_accuracy: 0.6998\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9917 - accuracy: 0.6982\n",
            "Epoch 7: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9911 - accuracy: 0.6986 - val_loss: 0.9746 - val_accuracy: 0.7006\n",
            "Epoch 8/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9868 - accuracy: 0.7010\n",
            "Epoch 8: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9870 - accuracy: 0.7011 - val_loss: 0.9724 - val_accuracy: 0.7010\n",
            "Epoch 9/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9850 - accuracy: 0.7007\n",
            "Epoch 9: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9851 - accuracy: 0.7009 - val_loss: 0.9703 - val_accuracy: 0.7006\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9826 - accuracy: 0.7034\n",
            "Epoch 10: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9828 - accuracy: 0.7032 - val_loss: 0.9676 - val_accuracy: 0.7053\n",
            "Epoch 11/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9827 - accuracy: 0.7033\n",
            "Epoch 11: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9827 - accuracy: 0.7035 - val_loss: 0.9681 - val_accuracy: 0.7030\n",
            "Epoch 12/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9790 - accuracy: 0.7062\n",
            "Epoch 12: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9790 - accuracy: 0.7063 - val_loss: 0.9681 - val_accuracy: 0.7069\n",
            "Epoch 13/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9804 - accuracy: 0.7036\n",
            "Epoch 13: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9805 - accuracy: 0.7035 - val_loss: 0.9660 - val_accuracy: 0.7034\n",
            "Epoch 14/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9754 - accuracy: 0.7058\n",
            "Epoch 14: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9756 - accuracy: 0.7058 - val_loss: 0.9662 - val_accuracy: 0.7081\n",
            "Epoch 15/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9730 - accuracy: 0.7098\n",
            "Epoch 15: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9730 - accuracy: 0.7098 - val_loss: 0.9617 - val_accuracy: 0.7077\n",
            "Epoch 16/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9732 - accuracy: 0.7087\n",
            "Epoch 16: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9732 - accuracy: 0.7087 - val_loss: 0.9627 - val_accuracy: 0.7089\n",
            "Epoch 17/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9738 - accuracy: 0.7077\n",
            "Epoch 17: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9724 - accuracy: 0.7084 - val_loss: 0.9629 - val_accuracy: 0.7093\n",
            "Epoch 18/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9725 - accuracy: 0.7106\n",
            "Epoch 18: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9724 - accuracy: 0.7105 - val_loss: 0.9624 - val_accuracy: 0.7105\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9696 - accuracy: 0.7081\n",
            "Epoch 19: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9694 - accuracy: 0.7081 - val_loss: 0.9617 - val_accuracy: 0.7065\n",
            "Epoch 20/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9653 - accuracy: 0.7117\n",
            "Epoch 20: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9657 - accuracy: 0.7119 - val_loss: 0.9619 - val_accuracy: 0.7097\n",
            "Epoch 21/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9699 - accuracy: 0.7133\n",
            "Epoch 21: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9692 - accuracy: 0.7134 - val_loss: 0.9599 - val_accuracy: 0.7093\n",
            "Epoch 22/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9676 - accuracy: 0.7129\n",
            "Epoch 22: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9676 - accuracy: 0.7129 - val_loss: 0.9627 - val_accuracy: 0.7129\n",
            "Epoch 23/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9657 - accuracy: 0.7154\n",
            "Epoch 23: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9657 - accuracy: 0.7154 - val_loss: 0.9576 - val_accuracy: 0.7125\n",
            "Epoch 24/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9699 - accuracy: 0.7130\n",
            "Epoch 24: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9698 - accuracy: 0.7131 - val_loss: 0.9601 - val_accuracy: 0.7129\n",
            "Epoch 25/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9665 - accuracy: 0.7163\n",
            "Epoch 25: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9664 - accuracy: 0.7163 - val_loss: 0.9584 - val_accuracy: 0.7093\n",
            "Epoch 26/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9641 - accuracy: 0.7134\n",
            "Epoch 26: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9641 - accuracy: 0.7133 - val_loss: 0.9588 - val_accuracy: 0.7105\n",
            "Epoch 27/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9660 - accuracy: 0.7153\n",
            "Epoch 27: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9659 - accuracy: 0.7152 - val_loss: 0.9572 - val_accuracy: 0.7141\n",
            "Epoch 28/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9647 - accuracy: 0.7138\n",
            "Epoch 28: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9647 - accuracy: 0.7138 - val_loss: 0.9573 - val_accuracy: 0.7141\n",
            "Epoch 29/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9650 - accuracy: 0.7143\n",
            "Epoch 29: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9649 - accuracy: 0.7142 - val_loss: 0.9560 - val_accuracy: 0.7133\n",
            "Epoch 30/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9642 - accuracy: 0.7149\n",
            "Epoch 30: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9640 - accuracy: 0.7151 - val_loss: 0.9551 - val_accuracy: 0.7145\n",
            "Epoch 31/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9611 - accuracy: 0.7145\n",
            "Epoch 31: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9612 - accuracy: 0.7145 - val_loss: 0.9574 - val_accuracy: 0.7145\n",
            "Epoch 32/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9644 - accuracy: 0.7163\n",
            "Epoch 32: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9644 - accuracy: 0.7164 - val_loss: 0.9588 - val_accuracy: 0.7101\n",
            "Epoch 33/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9609 - accuracy: 0.7161\n",
            "Epoch 33: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9608 - accuracy: 0.7160 - val_loss: 0.9557 - val_accuracy: 0.7149\n",
            "Epoch 34/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9601 - accuracy: 0.7134\n",
            "Epoch 34: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9606 - accuracy: 0.7133 - val_loss: 0.9572 - val_accuracy: 0.7133\n",
            "Epoch 35/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9593 - accuracy: 0.7150\n",
            "Epoch 35: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9593 - accuracy: 0.7149 - val_loss: 0.9570 - val_accuracy: 0.7113\n",
            "Epoch 36/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9582 - accuracy: 0.7200\n",
            "Epoch 36: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9579 - accuracy: 0.7200 - val_loss: 0.9532 - val_accuracy: 0.7184\n",
            "Epoch 37/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9582 - accuracy: 0.7213\n",
            "Epoch 37: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9584 - accuracy: 0.7215 - val_loss: 0.9520 - val_accuracy: 0.7236\n",
            "Epoch 38/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9604 - accuracy: 0.7175\n",
            "Epoch 38: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9599 - accuracy: 0.7178 - val_loss: 0.9542 - val_accuracy: 0.7192\n",
            "Epoch 39/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9596 - accuracy: 0.7173\n",
            "Epoch 39: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9592 - accuracy: 0.7174 - val_loss: 0.9561 - val_accuracy: 0.7168\n",
            "Epoch 40/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9560 - accuracy: 0.7207\n",
            "Epoch 40: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9556 - accuracy: 0.7209 - val_loss: 0.9548 - val_accuracy: 0.7176\n",
            "Epoch 41/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9578 - accuracy: 0.7200\n",
            "Epoch 41: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9578 - accuracy: 0.7199 - val_loss: 0.9527 - val_accuracy: 0.7152\n",
            "Epoch 42/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9536 - accuracy: 0.7208\n",
            "Epoch 42: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9539 - accuracy: 0.7207 - val_loss: 0.9524 - val_accuracy: 0.7176\n",
            "Epoch 43/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9548 - accuracy: 0.7188\n",
            "Epoch 43: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9550 - accuracy: 0.7188 - val_loss: 0.9520 - val_accuracy: 0.7184\n",
            "Epoch 44/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9567 - accuracy: 0.7156\n",
            "Epoch 44: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9567 - accuracy: 0.7156 - val_loss: 0.9531 - val_accuracy: 0.7156\n",
            "Epoch 45/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9546 - accuracy: 0.7226\n",
            "Epoch 45: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9543 - accuracy: 0.7228 - val_loss: 0.9513 - val_accuracy: 0.7176\n",
            "Epoch 46/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9551 - accuracy: 0.7191\n",
            "Epoch 46: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9542 - accuracy: 0.7199 - val_loss: 0.9527 - val_accuracy: 0.7172\n",
            "Epoch 47/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9515 - accuracy: 0.7232\n",
            "Epoch 47: saving model to magnitude_pruning_itr_2.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9516 - accuracy: 0.7232 - val_loss: 0.9545 - val_accuracy: 0.7145\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9545 - accuracy: 0.7145\n",
            "Post retraining val loss: 0.9545465111732483 | val acc: 0.7144554257392883 | sparsity: 0.9615858790116253\n",
            "Epoch 1/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9597 - accuracy: 0.7178\n",
            "Epoch 1: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 5s 12ms/step - loss: 0.9592 - accuracy: 0.7181 - val_loss: 0.9560 - val_accuracy: 0.7149\n",
            "Epoch 2/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.7173\n",
            "Epoch 2: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9572 - accuracy: 0.7172 - val_loss: 0.9557 - val_accuracy: 0.7133\n",
            "Epoch 3/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9518 - accuracy: 0.7210\n",
            "Epoch 3: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9531 - accuracy: 0.7197 - val_loss: 0.9499 - val_accuracy: 0.7248\n",
            "Epoch 4/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9557 - accuracy: 0.7192\n",
            "Epoch 4: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9555 - accuracy: 0.7194 - val_loss: 0.9512 - val_accuracy: 0.7184\n",
            "Epoch 5/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9532 - accuracy: 0.7214\n",
            "Epoch 5: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9532 - accuracy: 0.7214 - val_loss: 0.9496 - val_accuracy: 0.7208\n",
            "Epoch 6/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9560 - accuracy: 0.7201\n",
            "Epoch 6: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9555 - accuracy: 0.7201 - val_loss: 0.9543 - val_accuracy: 0.7180\n",
            "Epoch 7/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9507 - accuracy: 0.7224\n",
            "Epoch 7: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9504 - accuracy: 0.7228 - val_loss: 0.9504 - val_accuracy: 0.7160\n",
            "Epoch 8/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9505 - accuracy: 0.7235\n",
            "Epoch 8: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9505 - accuracy: 0.7235 - val_loss: 0.9489 - val_accuracy: 0.7184\n",
            "Epoch 9/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9536 - accuracy: 0.7202\n",
            "Epoch 9: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9539 - accuracy: 0.7198 - val_loss: 0.9491 - val_accuracy: 0.7156\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9525 - accuracy: 0.7217\n",
            "Epoch 10: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9525 - accuracy: 0.7215 - val_loss: 0.9474 - val_accuracy: 0.7208\n",
            "Epoch 11/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9530 - accuracy: 0.7216\n",
            "Epoch 11: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9531 - accuracy: 0.7214 - val_loss: 0.9494 - val_accuracy: 0.7240\n",
            "Epoch 12/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9500 - accuracy: 0.7210\n",
            "Epoch 12: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9513 - accuracy: 0.7205 - val_loss: 0.9472 - val_accuracy: 0.7240\n",
            "Epoch 13/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9523 - accuracy: 0.7233\n",
            "Epoch 13: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9525 - accuracy: 0.7232 - val_loss: 0.9458 - val_accuracy: 0.7251\n",
            "Epoch 14/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9478 - accuracy: 0.7231\n",
            "Epoch 14: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9470 - accuracy: 0.7235 - val_loss: 0.9495 - val_accuracy: 0.7172\n",
            "Epoch 15/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9475 - accuracy: 0.7262\n",
            "Epoch 15: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9476 - accuracy: 0.7259 - val_loss: 0.9470 - val_accuracy: 0.7204\n",
            "Epoch 16/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9513 - accuracy: 0.7224\n",
            "Epoch 16: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9506 - accuracy: 0.7229 - val_loss: 0.9484 - val_accuracy: 0.7192\n",
            "Epoch 17/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9475 - accuracy: 0.7239\n",
            "Epoch 17: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9474 - accuracy: 0.7239 - val_loss: 0.9482 - val_accuracy: 0.7196\n",
            "Epoch 18/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9449 - accuracy: 0.7271\n",
            "Epoch 18: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9447 - accuracy: 0.7271 - val_loss: 0.9471 - val_accuracy: 0.7204\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9474 - accuracy: 0.7196\n",
            "Epoch 19: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9477 - accuracy: 0.7196 - val_loss: 0.9475 - val_accuracy: 0.7196\n",
            "Epoch 20/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9497 - accuracy: 0.7208\n",
            "Epoch 20: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9502 - accuracy: 0.7206 - val_loss: 0.9486 - val_accuracy: 0.7212\n",
            "Epoch 21/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9473 - accuracy: 0.7260\n",
            "Epoch 21: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9477 - accuracy: 0.7259 - val_loss: 0.9502 - val_accuracy: 0.7156\n",
            "Epoch 22/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9418 - accuracy: 0.7252\n",
            "Epoch 22: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9418 - accuracy: 0.7253 - val_loss: 0.9482 - val_accuracy: 0.7216\n",
            "Epoch 23/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9454 - accuracy: 0.7227\n",
            "Epoch 23: saving model to magnitude_pruning_itr_3.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9457 - accuracy: 0.7224 - val_loss: 0.9447 - val_accuracy: 0.7240\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9447 - accuracy: 0.7240\n",
            "Post retraining val loss: 0.9447094202041626 | val acc: 0.7239603996276855 | sparsity: 0.9638711287784624\n",
            "Epoch 1/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9479 - accuracy: 0.7253\n",
            "Epoch 1: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 4s 11ms/step - loss: 0.9478 - accuracy: 0.7252 - val_loss: 0.9453 - val_accuracy: 0.7248\n",
            "Epoch 2/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9486 - accuracy: 0.7218\n",
            "Epoch 2: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9483 - accuracy: 0.7220 - val_loss: 0.9471 - val_accuracy: 0.7212\n",
            "Epoch 3/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9470 - accuracy: 0.7207\n",
            "Epoch 3: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9471 - accuracy: 0.7208 - val_loss: 0.9445 - val_accuracy: 0.7212\n",
            "Epoch 4/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9468 - accuracy: 0.7229\n",
            "Epoch 4: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9460 - accuracy: 0.7235 - val_loss: 0.9441 - val_accuracy: 0.7255\n",
            "Epoch 5/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9499 - accuracy: 0.7192\n",
            "Epoch 5: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9499 - accuracy: 0.7195 - val_loss: 0.9441 - val_accuracy: 0.7236\n",
            "Epoch 6/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9440 - accuracy: 0.7249\n",
            "Epoch 6: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9440 - accuracy: 0.7249 - val_loss: 0.9426 - val_accuracy: 0.7232\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9478 - accuracy: 0.7221\n",
            "Epoch 7: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9479 - accuracy: 0.7219 - val_loss: 0.9416 - val_accuracy: 0.7240\n",
            "Epoch 8/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9481 - accuracy: 0.7246\n",
            "Epoch 8: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9479 - accuracy: 0.7244 - val_loss: 0.9488 - val_accuracy: 0.7200\n",
            "Epoch 9/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9423 - accuracy: 0.7293\n",
            "Epoch 9: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9421 - accuracy: 0.7294 - val_loss: 0.9435 - val_accuracy: 0.7216\n",
            "Epoch 10/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9443 - accuracy: 0.7248\n",
            "Epoch 10: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9453 - accuracy: 0.7240 - val_loss: 0.9471 - val_accuracy: 0.7184\n",
            "Epoch 11/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9463 - accuracy: 0.7249\n",
            "Epoch 11: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9463 - accuracy: 0.7248 - val_loss: 0.9435 - val_accuracy: 0.7263\n",
            "Epoch 12/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9418 - accuracy: 0.7310\n",
            "Epoch 12: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9420 - accuracy: 0.7308 - val_loss: 0.9414 - val_accuracy: 0.7255\n",
            "Epoch 13/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9446 - accuracy: 0.7250\n",
            "Epoch 13: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9447 - accuracy: 0.7249 - val_loss: 0.9410 - val_accuracy: 0.7248\n",
            "Epoch 14/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9389 - accuracy: 0.7276\n",
            "Epoch 14: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9393 - accuracy: 0.7275 - val_loss: 0.9428 - val_accuracy: 0.7255\n",
            "Epoch 15/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9408 - accuracy: 0.7254\n",
            "Epoch 15: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9407 - accuracy: 0.7253 - val_loss: 0.9429 - val_accuracy: 0.7263\n",
            "Epoch 16/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9409 - accuracy: 0.7281\n",
            "Epoch 16: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9415 - accuracy: 0.7274 - val_loss: 0.9414 - val_accuracy: 0.7251\n",
            "Epoch 17/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9386 - accuracy: 0.7285\n",
            "Epoch 17: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 11ms/step - loss: 0.9384 - accuracy: 0.7285 - val_loss: 0.9418 - val_accuracy: 0.7248\n",
            "Epoch 18/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9422 - accuracy: 0.7283\n",
            "Epoch 18: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9420 - accuracy: 0.7287 - val_loss: 0.9400 - val_accuracy: 0.7263\n",
            "Epoch 19/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9382 - accuracy: 0.7279\n",
            "Epoch 19: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9386 - accuracy: 0.7277 - val_loss: 0.9401 - val_accuracy: 0.7275\n",
            "Epoch 20/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9391 - accuracy: 0.7281\n",
            "Epoch 20: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9389 - accuracy: 0.7284 - val_loss: 0.9424 - val_accuracy: 0.7224\n",
            "Epoch 21/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9419 - accuracy: 0.7292\n",
            "Epoch 21: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9417 - accuracy: 0.7292 - val_loss: 0.9420 - val_accuracy: 0.7244\n",
            "Epoch 22/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9378 - accuracy: 0.7313\n",
            "Epoch 22: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9382 - accuracy: 0.7311 - val_loss: 0.9392 - val_accuracy: 0.7271\n",
            "Epoch 23/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9413 - accuracy: 0.7256\n",
            "Epoch 23: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9416 - accuracy: 0.7253 - val_loss: 0.9399 - val_accuracy: 0.7263\n",
            "Epoch 24/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9409 - accuracy: 0.7278\n",
            "Epoch 24: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9410 - accuracy: 0.7277 - val_loss: 0.9380 - val_accuracy: 0.7307\n",
            "Epoch 25/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9391 - accuracy: 0.7297\n",
            "Epoch 25: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9393 - accuracy: 0.7297 - val_loss: 0.9396 - val_accuracy: 0.7255\n",
            "Epoch 26/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9380 - accuracy: 0.7307\n",
            "Epoch 26: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9379 - accuracy: 0.7306 - val_loss: 0.9431 - val_accuracy: 0.7255\n",
            "Epoch 27/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9408 - accuracy: 0.7280\n",
            "Epoch 27: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9397 - accuracy: 0.7282 - val_loss: 0.9394 - val_accuracy: 0.7263\n",
            "Epoch 28/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9408 - accuracy: 0.7263\n",
            "Epoch 28: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9405 - accuracy: 0.7265 - val_loss: 0.9422 - val_accuracy: 0.7251\n",
            "Epoch 29/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9393 - accuracy: 0.7261\n",
            "Epoch 29: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9392 - accuracy: 0.7261 - val_loss: 0.9395 - val_accuracy: 0.7251\n",
            "Epoch 30/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9402 - accuracy: 0.7281\n",
            "Epoch 30: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9391 - accuracy: 0.7286 - val_loss: 0.9381 - val_accuracy: 0.7291\n",
            "Epoch 31/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9372 - accuracy: 0.7293\n",
            "Epoch 31: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9371 - accuracy: 0.7297 - val_loss: 0.9406 - val_accuracy: 0.7251\n",
            "Epoch 32/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9390 - accuracy: 0.7266\n",
            "Epoch 32: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9391 - accuracy: 0.7264 - val_loss: 0.9410 - val_accuracy: 0.7275\n",
            "Epoch 33/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9375 - accuracy: 0.7301\n",
            "Epoch 33: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9373 - accuracy: 0.7304 - val_loss: 0.9388 - val_accuracy: 0.7279\n",
            "Epoch 34/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9402 - accuracy: 0.7295\n",
            "Epoch 34: saving model to magnitude_pruning_itr_4.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9401 - accuracy: 0.7296 - val_loss: 0.9402 - val_accuracy: 0.7263\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9402 - accuracy: 0.7263\n",
            "Post retraining val loss: 0.9402380585670471 | val acc: 0.726336658000946 | sparsity: 0.9647633037796851\n",
            "Epoch 1/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9410 - accuracy: 0.7271\n",
            "Epoch 1: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 4s 12ms/step - loss: 0.9413 - accuracy: 0.7267 - val_loss: 0.9400 - val_accuracy: 0.7248\n",
            "Epoch 2/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9423 - accuracy: 0.7256\n",
            "Epoch 2: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9424 - accuracy: 0.7255 - val_loss: 0.9400 - val_accuracy: 0.7279\n",
            "Epoch 3/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9365 - accuracy: 0.7299\n",
            "Epoch 3: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9366 - accuracy: 0.7298 - val_loss: 0.9386 - val_accuracy: 0.7291\n",
            "Epoch 4/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9441 - accuracy: 0.7249\n",
            "Epoch 4: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9444 - accuracy: 0.7249 - val_loss: 0.9406 - val_accuracy: 0.7240\n",
            "Epoch 5/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9392 - accuracy: 0.7296\n",
            "Epoch 5: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9388 - accuracy: 0.7298 - val_loss: 0.9370 - val_accuracy: 0.7287\n",
            "Epoch 6/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9350 - accuracy: 0.7297\n",
            "Epoch 6: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9352 - accuracy: 0.7297 - val_loss: 0.9406 - val_accuracy: 0.7224\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9371 - accuracy: 0.7292\n",
            "Epoch 7: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9370 - accuracy: 0.7293 - val_loss: 0.9386 - val_accuracy: 0.7299\n",
            "Epoch 8/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9380 - accuracy: 0.7277\n",
            "Epoch 8: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9380 - accuracy: 0.7284 - val_loss: 0.9402 - val_accuracy: 0.7251\n",
            "Epoch 9/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9374 - accuracy: 0.7315\n",
            "Epoch 9: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9372 - accuracy: 0.7313 - val_loss: 0.9371 - val_accuracy: 0.7295\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9350 - accuracy: 0.7329\n",
            "Epoch 10: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9350 - accuracy: 0.7329 - val_loss: 0.9388 - val_accuracy: 0.7248\n",
            "Epoch 11/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9391 - accuracy: 0.7309\n",
            "Epoch 11: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9391 - accuracy: 0.7304 - val_loss: 0.9392 - val_accuracy: 0.7244\n",
            "Epoch 12/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9370 - accuracy: 0.7281\n",
            "Epoch 12: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9370 - accuracy: 0.7280 - val_loss: 0.9377 - val_accuracy: 0.7275\n",
            "Epoch 13/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9352 - accuracy: 0.7322\n",
            "Epoch 13: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9346 - accuracy: 0.7326 - val_loss: 0.9386 - val_accuracy: 0.7279\n",
            "Epoch 14/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9362 - accuracy: 0.7314\n",
            "Epoch 14: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9370 - accuracy: 0.7305 - val_loss: 0.9389 - val_accuracy: 0.7267\n",
            "Epoch 15/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9408 - accuracy: 0.7270\n",
            "Epoch 15: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9412 - accuracy: 0.7267 - val_loss: 0.9367 - val_accuracy: 0.7275\n",
            "Epoch 16/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9321 - accuracy: 0.7340\n",
            "Epoch 16: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9332 - accuracy: 0.7334 - val_loss: 0.9399 - val_accuracy: 0.7267\n",
            "Epoch 17/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9365 - accuracy: 0.7294\n",
            "Epoch 17: saving model to magnitude_pruning_itr_5.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9367 - accuracy: 0.7294 - val_loss: 0.9380 - val_accuracy: 0.7263\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9380 - accuracy: 0.7263\n",
            "Post retraining val loss: 0.93802809715271 | val acc: 0.726336658000946 | sparsity: 0.9660298887058065\n",
            "Epoch 1/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9415 - accuracy: 0.7273\n",
            "Epoch 1: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 4s 11ms/step - loss: 0.9413 - accuracy: 0.7276 - val_loss: 0.9366 - val_accuracy: 0.7287\n",
            "Epoch 2/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9411 - accuracy: 0.7277\n",
            "Epoch 2: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9409 - accuracy: 0.7278 - val_loss: 0.9362 - val_accuracy: 0.7311\n",
            "Epoch 3/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9391 - accuracy: 0.7299\n",
            "Epoch 3: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9391 - accuracy: 0.7299 - val_loss: 0.9382 - val_accuracy: 0.7275\n",
            "Epoch 4/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9366 - accuracy: 0.7321\n",
            "Epoch 4: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9364 - accuracy: 0.7324 - val_loss: 0.9397 - val_accuracy: 0.7271\n",
            "Epoch 5/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9375 - accuracy: 0.7293\n",
            "Epoch 5: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9373 - accuracy: 0.7295 - val_loss: 0.9372 - val_accuracy: 0.7291\n",
            "Epoch 6/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9363 - accuracy: 0.7321\n",
            "Epoch 6: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9362 - accuracy: 0.7322 - val_loss: 0.9370 - val_accuracy: 0.7315\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9379 - accuracy: 0.7303\n",
            "Epoch 7: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9382 - accuracy: 0.7301 - val_loss: 0.9387 - val_accuracy: 0.7271\n",
            "Epoch 8/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9343 - accuracy: 0.7309\n",
            "Epoch 8: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9349 - accuracy: 0.7305 - val_loss: 0.9354 - val_accuracy: 0.7271\n",
            "Epoch 9/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9329 - accuracy: 0.7315\n",
            "Epoch 9: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9330 - accuracy: 0.7315 - val_loss: 0.9381 - val_accuracy: 0.7331\n",
            "Epoch 10/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9309 - accuracy: 0.7329\n",
            "Epoch 10: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9318 - accuracy: 0.7324 - val_loss: 0.9405 - val_accuracy: 0.7248\n",
            "Epoch 11/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9295 - accuracy: 0.7363\n",
            "Epoch 11: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9294 - accuracy: 0.7363 - val_loss: 0.9344 - val_accuracy: 0.7311\n",
            "Epoch 12/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9331 - accuracy: 0.7339\n",
            "Epoch 12: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9331 - accuracy: 0.7337 - val_loss: 0.9357 - val_accuracy: 0.7319\n",
            "Epoch 13/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9374 - accuracy: 0.7275\n",
            "Epoch 13: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9372 - accuracy: 0.7278 - val_loss: 0.9356 - val_accuracy: 0.7299\n",
            "Epoch 14/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9365 - accuracy: 0.7291\n",
            "Epoch 14: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9364 - accuracy: 0.7292 - val_loss: 0.9362 - val_accuracy: 0.7295\n",
            "Epoch 15/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9324 - accuracy: 0.7362\n",
            "Epoch 15: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9325 - accuracy: 0.7358 - val_loss: 0.9357 - val_accuracy: 0.7335\n",
            "Epoch 16/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9346 - accuracy: 0.7315\n",
            "Epoch 16: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9345 - accuracy: 0.7316 - val_loss: 0.9359 - val_accuracy: 0.7319\n",
            "Epoch 17/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9355 - accuracy: 0.7312\n",
            "Epoch 17: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 11ms/step - loss: 0.9355 - accuracy: 0.7312 - val_loss: 0.9353 - val_accuracy: 0.7335\n",
            "Epoch 18/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9342 - accuracy: 0.7285\n",
            "Epoch 18: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9341 - accuracy: 0.7287 - val_loss: 0.9341 - val_accuracy: 0.7327\n",
            "Epoch 19/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.7345\n",
            "Epoch 19: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9305 - accuracy: 0.7347 - val_loss: 0.9354 - val_accuracy: 0.7327\n",
            "Epoch 20/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9291 - accuracy: 0.7321\n",
            "Epoch 20: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9295 - accuracy: 0.7312 - val_loss: 0.9368 - val_accuracy: 0.7295\n",
            "Epoch 21/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9361 - accuracy: 0.7276\n",
            "Epoch 21: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9355 - accuracy: 0.7281 - val_loss: 0.9350 - val_accuracy: 0.7311\n",
            "Epoch 22/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9299 - accuracy: 0.7351\n",
            "Epoch 22: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9298 - accuracy: 0.7353 - val_loss: 0.9342 - val_accuracy: 0.7343\n",
            "Epoch 23/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9333 - accuracy: 0.7355\n",
            "Epoch 23: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9338 - accuracy: 0.7351 - val_loss: 0.9367 - val_accuracy: 0.7287\n",
            "Epoch 24/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9321 - accuracy: 0.7313\n",
            "Epoch 24: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9321 - accuracy: 0.7308 - val_loss: 0.9343 - val_accuracy: 0.7299\n",
            "Epoch 25/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9332 - accuracy: 0.7318\n",
            "Epoch 25: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9333 - accuracy: 0.7317 - val_loss: 0.9352 - val_accuracy: 0.7323\n",
            "Epoch 26/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9312 - accuracy: 0.7350\n",
            "Epoch 26: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9311 - accuracy: 0.7350 - val_loss: 0.9365 - val_accuracy: 0.7319\n",
            "Epoch 27/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9303 - accuracy: 0.7309\n",
            "Epoch 27: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9306 - accuracy: 0.7307 - val_loss: 0.9359 - val_accuracy: 0.7295\n",
            "Epoch 28/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9315 - accuracy: 0.7348\n",
            "Epoch 28: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9314 - accuracy: 0.7349 - val_loss: 0.9330 - val_accuracy: 0.7307\n",
            "Epoch 29/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9363 - accuracy: 0.7290\n",
            "Epoch 29: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9360 - accuracy: 0.7293 - val_loss: 0.9359 - val_accuracy: 0.7319\n",
            "Epoch 30/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9309 - accuracy: 0.7356\n",
            "Epoch 30: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9307 - accuracy: 0.7355 - val_loss: 0.9360 - val_accuracy: 0.7295\n",
            "Epoch 31/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9333 - accuracy: 0.7323\n",
            "Epoch 31: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9334 - accuracy: 0.7325 - val_loss: 0.9361 - val_accuracy: 0.7350\n",
            "Epoch 32/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9328 - accuracy: 0.7334\n",
            "Epoch 32: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9331 - accuracy: 0.7332 - val_loss: 0.9358 - val_accuracy: 0.7319\n",
            "Epoch 33/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9289 - accuracy: 0.7320\n",
            "Epoch 33: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9289 - accuracy: 0.7319 - val_loss: 0.9350 - val_accuracy: 0.7291\n",
            "Epoch 34/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9336 - accuracy: 0.7353\n",
            "Epoch 34: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9335 - accuracy: 0.7355 - val_loss: 0.9336 - val_accuracy: 0.7347\n",
            "Epoch 35/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9265 - accuracy: 0.7357\n",
            "Epoch 35: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9265 - accuracy: 0.7358 - val_loss: 0.9347 - val_accuracy: 0.7319\n",
            "Epoch 36/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9325 - accuracy: 0.7353\n",
            "Epoch 36: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9327 - accuracy: 0.7349 - val_loss: 0.9335 - val_accuracy: 0.7343\n",
            "Epoch 37/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9328 - accuracy: 0.7359\n",
            "Epoch 37: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9328 - accuracy: 0.7359 - val_loss: 0.9318 - val_accuracy: 0.7335\n",
            "Epoch 38/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9318 - accuracy: 0.7325\n",
            "Epoch 38: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9318 - accuracy: 0.7325 - val_loss: 0.9323 - val_accuracy: 0.7335\n",
            "Epoch 39/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9285 - accuracy: 0.7342\n",
            "Epoch 39: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9286 - accuracy: 0.7341 - val_loss: 0.9366 - val_accuracy: 0.7307\n",
            "Epoch 40/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9300 - accuracy: 0.7358\n",
            "Epoch 40: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9297 - accuracy: 0.7360 - val_loss: 0.9322 - val_accuracy: 0.7323\n",
            "Epoch 41/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9300 - accuracy: 0.7342\n",
            "Epoch 41: saving model to magnitude_pruning_itr_6.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9303 - accuracy: 0.7341 - val_loss: 0.9344 - val_accuracy: 0.7347\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9344 - accuracy: 0.7347\n",
            "Post retraining val loss: 0.9343589544296265 | val acc: 0.7346534729003906 | sparsity: 0.9666168015610532\n",
            "Epoch 1/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9469 - accuracy: 0.7263\n",
            "Epoch 1: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 4s 11ms/step - loss: 0.9469 - accuracy: 0.7262 - val_loss: 0.9420 - val_accuracy: 0.7307\n",
            "Epoch 2/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9391 - accuracy: 0.7284\n",
            "Epoch 2: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9389 - accuracy: 0.7285 - val_loss: 0.9375 - val_accuracy: 0.7291\n",
            "Epoch 3/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9337 - accuracy: 0.7323\n",
            "Epoch 3: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9336 - accuracy: 0.7325 - val_loss: 0.9426 - val_accuracy: 0.7251\n",
            "Epoch 4/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9377 - accuracy: 0.7283\n",
            "Epoch 4: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9373 - accuracy: 0.7282 - val_loss: 0.9411 - val_accuracy: 0.7287\n",
            "Epoch 5/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9300 - accuracy: 0.7337\n",
            "Epoch 5: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9301 - accuracy: 0.7338 - val_loss: 0.9355 - val_accuracy: 0.7327\n",
            "Epoch 6/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9318 - accuracy: 0.7365\n",
            "Epoch 6: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9320 - accuracy: 0.7364 - val_loss: 0.9370 - val_accuracy: 0.7311\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9368 - accuracy: 0.7321\n",
            "Epoch 7: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9370 - accuracy: 0.7321 - val_loss: 0.9384 - val_accuracy: 0.7267\n",
            "Epoch 8/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9292 - accuracy: 0.7379\n",
            "Epoch 8: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9294 - accuracy: 0.7376 - val_loss: 0.9339 - val_accuracy: 0.7323\n",
            "Epoch 9/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9332 - accuracy: 0.7312\n",
            "Epoch 9: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9334 - accuracy: 0.7311 - val_loss: 0.9387 - val_accuracy: 0.7248\n",
            "Epoch 10/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9332 - accuracy: 0.7329\n",
            "Epoch 10: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9330 - accuracy: 0.7330 - val_loss: 0.9396 - val_accuracy: 0.7275\n",
            "Epoch 11/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.7347\n",
            "Epoch 11: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9304 - accuracy: 0.7346 - val_loss: 0.9357 - val_accuracy: 0.7307\n",
            "Epoch 12/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9348 - accuracy: 0.7307\n",
            "Epoch 12: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9349 - accuracy: 0.7308 - val_loss: 0.9379 - val_accuracy: 0.7303\n",
            "Epoch 13/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.7321\n",
            "Epoch 13: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9308 - accuracy: 0.7317 - val_loss: 0.9367 - val_accuracy: 0.7319\n",
            "Epoch 14/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9330 - accuracy: 0.7329\n",
            "Epoch 14: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9330 - accuracy: 0.7329 - val_loss: 0.9337 - val_accuracy: 0.7343\n",
            "Epoch 15/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9319 - accuracy: 0.7320\n",
            "Epoch 15: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9319 - accuracy: 0.7320 - val_loss: 0.9336 - val_accuracy: 0.7323\n",
            "Epoch 16/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9279 - accuracy: 0.7355\n",
            "Epoch 16: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9282 - accuracy: 0.7357 - val_loss: 0.9334 - val_accuracy: 0.7347\n",
            "Epoch 17/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9289 - accuracy: 0.7323\n",
            "Epoch 17: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9288 - accuracy: 0.7325 - val_loss: 0.9325 - val_accuracy: 0.7347\n",
            "Epoch 18/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9297 - accuracy: 0.7371\n",
            "Epoch 18: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9298 - accuracy: 0.7369 - val_loss: 0.9345 - val_accuracy: 0.7315\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9225 - accuracy: 0.7410\n",
            "Epoch 19: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9228 - accuracy: 0.7408 - val_loss: 0.9358 - val_accuracy: 0.7307\n",
            "Epoch 20/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9270 - accuracy: 0.7375\n",
            "Epoch 20: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9271 - accuracy: 0.7378 - val_loss: 0.9347 - val_accuracy: 0.7343\n",
            "Epoch 21/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9274 - accuracy: 0.7359\n",
            "Epoch 21: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9276 - accuracy: 0.7359 - val_loss: 0.9326 - val_accuracy: 0.7335\n",
            "Epoch 22/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9274 - accuracy: 0.7337\n",
            "Epoch 22: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9274 - accuracy: 0.7337 - val_loss: 0.9347 - val_accuracy: 0.7307\n",
            "Epoch 23/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9345 - accuracy: 0.7319\n",
            "Epoch 23: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9345 - accuracy: 0.7319 - val_loss: 0.9363 - val_accuracy: 0.7299\n",
            "Epoch 24/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9297 - accuracy: 0.7359\n",
            "Epoch 24: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9296 - accuracy: 0.7359 - val_loss: 0.9366 - val_accuracy: 0.7267\n",
            "Epoch 25/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9277 - accuracy: 0.7357\n",
            "Epoch 25: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9274 - accuracy: 0.7360 - val_loss: 0.9322 - val_accuracy: 0.7299\n",
            "Epoch 26/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9269 - accuracy: 0.7344\n",
            "Epoch 26: saving model to magnitude_pruning_itr_7.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9271 - accuracy: 0.7340 - val_loss: 0.9318 - val_accuracy: 0.7319\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9318 - accuracy: 0.7319\n",
            "Post retraining val loss: 0.9318239688873291 | val acc: 0.7318812012672424 | sparsity: 0.9677619562412617\n",
            "Epoch 1/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9337 - accuracy: 0.7322\n",
            "Epoch 1: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 5s 12ms/step - loss: 0.9335 - accuracy: 0.7325 - val_loss: 0.9333 - val_accuracy: 0.7331\n",
            "Epoch 2/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9320 - accuracy: 0.7324\n",
            "Epoch 2: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9325 - accuracy: 0.7322 - val_loss: 0.9364 - val_accuracy: 0.7339\n",
            "Epoch 3/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9317 - accuracy: 0.7342\n",
            "Epoch 3: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9321 - accuracy: 0.7337 - val_loss: 0.9354 - val_accuracy: 0.7315\n",
            "Epoch 4/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9299 - accuracy: 0.7369\n",
            "Epoch 4: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9301 - accuracy: 0.7368 - val_loss: 0.9346 - val_accuracy: 0.7335\n",
            "Epoch 5/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9336 - accuracy: 0.7338\n",
            "Epoch 5: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9333 - accuracy: 0.7340 - val_loss: 0.9354 - val_accuracy: 0.7335\n",
            "Epoch 6/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9322 - accuracy: 0.7325\n",
            "Epoch 6: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9320 - accuracy: 0.7326 - val_loss: 0.9332 - val_accuracy: 0.7335\n",
            "Epoch 7/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9286 - accuracy: 0.7332\n",
            "Epoch 7: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9289 - accuracy: 0.7330 - val_loss: 0.9351 - val_accuracy: 0.7339\n",
            "Epoch 8/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9281 - accuracy: 0.7316\n",
            "Epoch 8: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9275 - accuracy: 0.7321 - val_loss: 0.9332 - val_accuracy: 0.7307\n",
            "Epoch 9/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9295 - accuracy: 0.7339\n",
            "Epoch 9: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9293 - accuracy: 0.7339 - val_loss: 0.9354 - val_accuracy: 0.7323\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9293 - accuracy: 0.7348\n",
            "Epoch 10: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9296 - accuracy: 0.7345 - val_loss: 0.9351 - val_accuracy: 0.7350\n",
            "Epoch 11/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9268 - accuracy: 0.7341\n",
            "Epoch 11: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9266 - accuracy: 0.7342 - val_loss: 0.9358 - val_accuracy: 0.7331\n",
            "Epoch 12/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9286 - accuracy: 0.7356\n",
            "Epoch 12: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9281 - accuracy: 0.7357 - val_loss: 0.9331 - val_accuracy: 0.7339\n",
            "Epoch 13/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9258 - accuracy: 0.7358\n",
            "Epoch 13: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9259 - accuracy: 0.7356 - val_loss: 0.9348 - val_accuracy: 0.7335\n",
            "Epoch 14/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9212 - accuracy: 0.7426\n",
            "Epoch 14: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9213 - accuracy: 0.7426 - val_loss: 0.9340 - val_accuracy: 0.7303\n",
            "Epoch 15/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9253 - accuracy: 0.7388\n",
            "Epoch 15: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9255 - accuracy: 0.7387 - val_loss: 0.9362 - val_accuracy: 0.7311\n",
            "Epoch 16/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9257 - accuracy: 0.7384\n",
            "Epoch 16: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9258 - accuracy: 0.7386 - val_loss: 0.9347 - val_accuracy: 0.7315\n",
            "Epoch 17/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9267 - accuracy: 0.7385\n",
            "Epoch 17: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9271 - accuracy: 0.7385 - val_loss: 0.9357 - val_accuracy: 0.7323\n",
            "Epoch 18/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9276 - accuracy: 0.7390\n",
            "Epoch 18: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9274 - accuracy: 0.7392 - val_loss: 0.9336 - val_accuracy: 0.7315\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9268 - accuracy: 0.7365\n",
            "Epoch 19: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9266 - accuracy: 0.7367 - val_loss: 0.9338 - val_accuracy: 0.7339\n",
            "Epoch 20/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9278 - accuracy: 0.7338\n",
            "Epoch 20: saving model to magnitude_pruning_itr_8.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9280 - accuracy: 0.7336 - val_loss: 0.9345 - val_accuracy: 0.7339\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9345 - accuracy: 0.7339\n",
            "Post retraining val loss: 0.9345096945762634 | val acc: 0.7338613867759705 | sparsity: 0.968367420939634\n",
            "Epoch 1/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9350 - accuracy: 0.7318\n",
            "Epoch 1: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 5s 11ms/step - loss: 0.9348 - accuracy: 0.7317 - val_loss: 0.9362 - val_accuracy: 0.7311\n",
            "Epoch 2/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9295 - accuracy: 0.7326\n",
            "Epoch 2: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9292 - accuracy: 0.7327 - val_loss: 0.9355 - val_accuracy: 0.7315\n",
            "Epoch 3/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9271 - accuracy: 0.7363\n",
            "Epoch 3: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9269 - accuracy: 0.7366 - val_loss: 0.9366 - val_accuracy: 0.7271\n",
            "Epoch 4/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9238 - accuracy: 0.7398\n",
            "Epoch 4: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9239 - accuracy: 0.7399 - val_loss: 0.9334 - val_accuracy: 0.7299\n",
            "Epoch 5/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9277 - accuracy: 0.7361\n",
            "Epoch 5: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9280 - accuracy: 0.7358 - val_loss: 0.9339 - val_accuracy: 0.7295\n",
            "Epoch 6/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9257 - accuracy: 0.7367\n",
            "Epoch 6: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9252 - accuracy: 0.7369 - val_loss: 0.9354 - val_accuracy: 0.7295\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.7338\n",
            "Epoch 7: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9306 - accuracy: 0.7338 - val_loss: 0.9331 - val_accuracy: 0.7327\n",
            "Epoch 8/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9282 - accuracy: 0.7354\n",
            "Epoch 8: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9280 - accuracy: 0.7354 - val_loss: 0.9322 - val_accuracy: 0.7335\n",
            "Epoch 9/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9251 - accuracy: 0.7369\n",
            "Epoch 9: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9250 - accuracy: 0.7368 - val_loss: 0.9314 - val_accuracy: 0.7347\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9242 - accuracy: 0.7383\n",
            "Epoch 10: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9244 - accuracy: 0.7380 - val_loss: 0.9344 - val_accuracy: 0.7283\n",
            "Epoch 11/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9245 - accuracy: 0.7376\n",
            "Epoch 11: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9248 - accuracy: 0.7374 - val_loss: 0.9333 - val_accuracy: 0.7331\n",
            "Epoch 12/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9295 - accuracy: 0.7347\n",
            "Epoch 12: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9304 - accuracy: 0.7341 - val_loss: 0.9315 - val_accuracy: 0.7339\n",
            "Epoch 13/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9262 - accuracy: 0.7350\n",
            "Epoch 13: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9260 - accuracy: 0.7354 - val_loss: 0.9317 - val_accuracy: 0.7311\n",
            "Epoch 14/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9271 - accuracy: 0.7347\n",
            "Epoch 14: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9271 - accuracy: 0.7348 - val_loss: 0.9329 - val_accuracy: 0.7331\n",
            "Epoch 15/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9252 - accuracy: 0.7395\n",
            "Epoch 15: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9251 - accuracy: 0.7394 - val_loss: 0.9330 - val_accuracy: 0.7350\n",
            "Epoch 16/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9260 - accuracy: 0.7375\n",
            "Epoch 16: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9260 - accuracy: 0.7376 - val_loss: 0.9315 - val_accuracy: 0.7339\n",
            "Epoch 17/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9267 - accuracy: 0.7358\n",
            "Epoch 17: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9268 - accuracy: 0.7358 - val_loss: 0.9337 - val_accuracy: 0.7299\n",
            "Epoch 18/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9279 - accuracy: 0.7366\n",
            "Epoch 18: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9275 - accuracy: 0.7370 - val_loss: 0.9312 - val_accuracy: 0.7327\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9280 - accuracy: 0.7355\n",
            "Epoch 19: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9278 - accuracy: 0.7357 - val_loss: 0.9321 - val_accuracy: 0.7347\n",
            "Epoch 20/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9307 - accuracy: 0.7343\n",
            "Epoch 20: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9305 - accuracy: 0.7345 - val_loss: 0.9306 - val_accuracy: 0.7374\n",
            "Epoch 21/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9277 - accuracy: 0.7361\n",
            "Epoch 21: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9273 - accuracy: 0.7366 - val_loss: 0.9319 - val_accuracy: 0.7339\n",
            "Epoch 22/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9217 - accuracy: 0.7422\n",
            "Epoch 22: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9226 - accuracy: 0.7419 - val_loss: 0.9315 - val_accuracy: 0.7307\n",
            "Epoch 23/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9298 - accuracy: 0.7353\n",
            "Epoch 23: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9299 - accuracy: 0.7353 - val_loss: 0.9292 - val_accuracy: 0.7366\n",
            "Epoch 24/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9283 - accuracy: 0.7367\n",
            "Epoch 24: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9283 - accuracy: 0.7369 - val_loss: 0.9311 - val_accuracy: 0.7335\n",
            "Epoch 25/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9232 - accuracy: 0.7407\n",
            "Epoch 25: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9227 - accuracy: 0.7408 - val_loss: 0.9311 - val_accuracy: 0.7319\n",
            "Epoch 26/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9273 - accuracy: 0.7407\n",
            "Epoch 26: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9274 - accuracy: 0.7406 - val_loss: 0.9319 - val_accuracy: 0.7366\n",
            "Epoch 27/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9229 - accuracy: 0.7397\n",
            "Epoch 27: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9230 - accuracy: 0.7393 - val_loss: 0.9297 - val_accuracy: 0.7358\n",
            "Epoch 28/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9244 - accuracy: 0.7407\n",
            "Epoch 28: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9245 - accuracy: 0.7405 - val_loss: 0.9291 - val_accuracy: 0.7331\n",
            "Epoch 29/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9260 - accuracy: 0.7381\n",
            "Epoch 29: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9266 - accuracy: 0.7378 - val_loss: 0.9320 - val_accuracy: 0.7319\n",
            "Epoch 30/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9292 - accuracy: 0.7354\n",
            "Epoch 30: saving model to magnitude_pruning_itr_9.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9290 - accuracy: 0.7356 - val_loss: 0.9299 - val_accuracy: 0.7354\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9299 - accuracy: 0.7354\n",
            "Post retraining val loss: 0.9298559427261353 | val acc: 0.7354455590248108 | sparsity: 0.9687974863939096\n",
            "Epoch 1/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9380 - accuracy: 0.7325\n",
            "Epoch 1: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 5s 12ms/step - loss: 0.9373 - accuracy: 0.7327 - val_loss: 0.9305 - val_accuracy: 0.7303\n",
            "Epoch 2/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9327 - accuracy: 0.7327\n",
            "Epoch 2: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9320 - accuracy: 0.7333 - val_loss: 0.9302 - val_accuracy: 0.7335\n",
            "Epoch 3/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9262 - accuracy: 0.7349\n",
            "Epoch 3: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9276 - accuracy: 0.7343 - val_loss: 0.9297 - val_accuracy: 0.7358\n",
            "Epoch 4/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9287 - accuracy: 0.7355\n",
            "Epoch 4: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9281 - accuracy: 0.7361 - val_loss: 0.9300 - val_accuracy: 0.7343\n",
            "Epoch 5/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9296 - accuracy: 0.7364\n",
            "Epoch 5: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9295 - accuracy: 0.7366 - val_loss: 0.9296 - val_accuracy: 0.7362\n",
            "Epoch 6/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9241 - accuracy: 0.7384\n",
            "Epoch 6: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9240 - accuracy: 0.7385 - val_loss: 0.9296 - val_accuracy: 0.7343\n",
            "Epoch 7/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9282 - accuracy: 0.7355\n",
            "Epoch 7: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9277 - accuracy: 0.7356 - val_loss: 0.9264 - val_accuracy: 0.7358\n",
            "Epoch 8/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9260 - accuracy: 0.7383\n",
            "Epoch 8: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 11ms/step - loss: 0.9260 - accuracy: 0.7383 - val_loss: 0.9267 - val_accuracy: 0.7347\n",
            "Epoch 9/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9260 - accuracy: 0.7363\n",
            "Epoch 9: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9256 - accuracy: 0.7365 - val_loss: 0.9322 - val_accuracy: 0.7335\n",
            "Epoch 10/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9234 - accuracy: 0.7382\n",
            "Epoch 10: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9233 - accuracy: 0.7384 - val_loss: 0.9279 - val_accuracy: 0.7374\n",
            "Epoch 11/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9280 - accuracy: 0.7347\n",
            "Epoch 11: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9282 - accuracy: 0.7343 - val_loss: 0.9306 - val_accuracy: 0.7362\n",
            "Epoch 12/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9264 - accuracy: 0.7355\n",
            "Epoch 12: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9264 - accuracy: 0.7354 - val_loss: 0.9285 - val_accuracy: 0.7362\n",
            "Epoch 13/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9257 - accuracy: 0.7374\n",
            "Epoch 13: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9253 - accuracy: 0.7373 - val_loss: 0.9293 - val_accuracy: 0.7350\n",
            "Epoch 14/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9248 - accuracy: 0.7352\n",
            "Epoch 14: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9248 - accuracy: 0.7352 - val_loss: 0.9281 - val_accuracy: 0.7358\n",
            "Epoch 15/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9251 - accuracy: 0.7356\n",
            "Epoch 15: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9251 - accuracy: 0.7358 - val_loss: 0.9277 - val_accuracy: 0.7362\n",
            "Epoch 16/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9267 - accuracy: 0.7325\n",
            "Epoch 16: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9266 - accuracy: 0.7326 - val_loss: 0.9296 - val_accuracy: 0.7374\n",
            "Epoch 17/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9245 - accuracy: 0.7373\n",
            "Epoch 17: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9249 - accuracy: 0.7373 - val_loss: 0.9277 - val_accuracy: 0.7374\n",
            "Epoch 18/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9223 - accuracy: 0.7389\n",
            "Epoch 18: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9223 - accuracy: 0.7392 - val_loss: 0.9303 - val_accuracy: 0.7331\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9248 - accuracy: 0.7352\n",
            "Epoch 19: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9249 - accuracy: 0.7351 - val_loss: 0.9292 - val_accuracy: 0.7350\n",
            "Epoch 20/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9263 - accuracy: 0.7356\n",
            "Epoch 20: saving model to magnitude_pruning_itr_10.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9266 - accuracy: 0.7355 - val_loss: 0.9296 - val_accuracy: 0.7343\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9296 - accuracy: 0.7343\n",
            "Post retraining val loss: 0.9296309351921082 | val acc: 0.7342574000358582 | sparsity: 0.9694417413097264\n",
            "Epoch 1/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9327 - accuracy: 0.7320\n",
            "Epoch 1: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 5s 11ms/step - loss: 0.9320 - accuracy: 0.7319 - val_loss: 0.9322 - val_accuracy: 0.7327\n",
            "Epoch 2/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.7325\n",
            "Epoch 2: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9305 - accuracy: 0.7326 - val_loss: 0.9309 - val_accuracy: 0.7358\n",
            "Epoch 3/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9264 - accuracy: 0.7386\n",
            "Epoch 3: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9262 - accuracy: 0.7389 - val_loss: 0.9319 - val_accuracy: 0.7327\n",
            "Epoch 4/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9212 - accuracy: 0.7362\n",
            "Epoch 4: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9214 - accuracy: 0.7358 - val_loss: 0.9305 - val_accuracy: 0.7350\n",
            "Epoch 5/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9301 - accuracy: 0.7337\n",
            "Epoch 5: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9302 - accuracy: 0.7339 - val_loss: 0.9288 - val_accuracy: 0.7362\n",
            "Epoch 6/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9290 - accuracy: 0.7341\n",
            "Epoch 6: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9288 - accuracy: 0.7339 - val_loss: 0.9302 - val_accuracy: 0.7335\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9233 - accuracy: 0.7366\n",
            "Epoch 7: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9233 - accuracy: 0.7367 - val_loss: 0.9281 - val_accuracy: 0.7354\n",
            "Epoch 8/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9249 - accuracy: 0.7358\n",
            "Epoch 8: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9245 - accuracy: 0.7359 - val_loss: 0.9291 - val_accuracy: 0.7362\n",
            "Epoch 9/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9211 - accuracy: 0.7385\n",
            "Epoch 9: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9211 - accuracy: 0.7386 - val_loss: 0.9298 - val_accuracy: 0.7386\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9257 - accuracy: 0.7386\n",
            "Epoch 10: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9262 - accuracy: 0.7383 - val_loss: 0.9316 - val_accuracy: 0.7339\n",
            "Epoch 11/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9249 - accuracy: 0.7411\n",
            "Epoch 11: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9247 - accuracy: 0.7412 - val_loss: 0.9326 - val_accuracy: 0.7311\n",
            "Epoch 12/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9271 - accuracy: 0.7363\n",
            "Epoch 12: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9269 - accuracy: 0.7365 - val_loss: 0.9278 - val_accuracy: 0.7347\n",
            "Epoch 13/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9208 - accuracy: 0.7389\n",
            "Epoch 13: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9209 - accuracy: 0.7389 - val_loss: 0.9286 - val_accuracy: 0.7350\n",
            "Epoch 14/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9240 - accuracy: 0.7353\n",
            "Epoch 14: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9239 - accuracy: 0.7353 - val_loss: 0.9292 - val_accuracy: 0.7335\n",
            "Epoch 15/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9277 - accuracy: 0.7354\n",
            "Epoch 15: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9278 - accuracy: 0.7355 - val_loss: 0.9280 - val_accuracy: 0.7350\n",
            "Epoch 16/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9201 - accuracy: 0.7408\n",
            "Epoch 16: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9199 - accuracy: 0.7406 - val_loss: 0.9314 - val_accuracy: 0.7335\n",
            "Epoch 17/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9228 - accuracy: 0.7388\n",
            "Epoch 17: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9233 - accuracy: 0.7386 - val_loss: 0.9281 - val_accuracy: 0.7354\n",
            "Epoch 18/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9198 - accuracy: 0.7400\n",
            "Epoch 18: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9198 - accuracy: 0.7401 - val_loss: 0.9275 - val_accuracy: 0.7378\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9226 - accuracy: 0.7399\n",
            "Epoch 19: saving model to magnitude_pruning_itr_11.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9227 - accuracy: 0.7398 - val_loss: 0.9285 - val_accuracy: 0.7331\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9285 - accuracy: 0.7331\n",
            "Post retraining val loss: 0.9285286068916321 | val acc: 0.7330693006515503 | sparsity: 0.9698431357337169\n",
            "Epoch 1/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9360 - accuracy: 0.7334\n",
            "Epoch 1: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 4s 11ms/step - loss: 0.9357 - accuracy: 0.7340 - val_loss: 0.9314 - val_accuracy: 0.7350\n",
            "Epoch 2/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9299 - accuracy: 0.7342\n",
            "Epoch 2: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9297 - accuracy: 0.7343 - val_loss: 0.9302 - val_accuracy: 0.7343\n",
            "Epoch 3/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9294 - accuracy: 0.7331\n",
            "Epoch 3: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9295 - accuracy: 0.7330 - val_loss: 0.9298 - val_accuracy: 0.7339\n",
            "Epoch 4/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9271 - accuracy: 0.7355\n",
            "Epoch 4: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9271 - accuracy: 0.7354 - val_loss: 0.9302 - val_accuracy: 0.7315\n",
            "Epoch 5/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9237 - accuracy: 0.7394\n",
            "Epoch 5: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9236 - accuracy: 0.7397 - val_loss: 0.9284 - val_accuracy: 0.7354\n",
            "Epoch 6/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9247 - accuracy: 0.7403\n",
            "Epoch 6: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9252 - accuracy: 0.7398 - val_loss: 0.9305 - val_accuracy: 0.7311\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9272 - accuracy: 0.7338\n",
            "Epoch 7: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9275 - accuracy: 0.7337 - val_loss: 0.9271 - val_accuracy: 0.7354\n",
            "Epoch 8/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9229 - accuracy: 0.7374\n",
            "Epoch 8: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9231 - accuracy: 0.7374 - val_loss: 0.9268 - val_accuracy: 0.7311\n",
            "Epoch 9/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9233 - accuracy: 0.7377\n",
            "Epoch 9: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9233 - accuracy: 0.7377 - val_loss: 0.9275 - val_accuracy: 0.7323\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9177 - accuracy: 0.7438\n",
            "Epoch 10: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9180 - accuracy: 0.7436 - val_loss: 0.9270 - val_accuracy: 0.7350\n",
            "Epoch 11/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9269 - accuracy: 0.7386\n",
            "Epoch 11: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9269 - accuracy: 0.7387 - val_loss: 0.9272 - val_accuracy: 0.7335\n",
            "Epoch 12/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9283 - accuracy: 0.7365\n",
            "Epoch 12: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9280 - accuracy: 0.7367 - val_loss: 0.9276 - val_accuracy: 0.7331\n",
            "Epoch 13/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9193 - accuracy: 0.7389\n",
            "Epoch 13: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9203 - accuracy: 0.7384 - val_loss: 0.9271 - val_accuracy: 0.7327\n",
            "Epoch 14/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9255 - accuracy: 0.7369\n",
            "Epoch 14: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9265 - accuracy: 0.7363 - val_loss: 0.9290 - val_accuracy: 0.7299\n",
            "Epoch 15/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9237 - accuracy: 0.7365\n",
            "Epoch 15: saving model to magnitude_pruning_itr_12.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9237 - accuracy: 0.7365 - val_loss: 0.9271 - val_accuracy: 0.7323\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9271 - accuracy: 0.7323\n",
            "Post retraining val loss: 0.9271082878112793 | val acc: 0.7322772145271301 | sparsity: 0.9702209187210022\n",
            "Epoch 1/50\n",
            "172/176 [============================>.] - ETA: 0s - loss: 0.9268 - accuracy: 0.7341\n",
            "Epoch 1: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 4s 11ms/step - loss: 0.9267 - accuracy: 0.7339 - val_loss: 0.9299 - val_accuracy: 0.7279\n",
            "Epoch 2/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9270 - accuracy: 0.7375\n",
            "Epoch 2: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9265 - accuracy: 0.7378 - val_loss: 0.9289 - val_accuracy: 0.7319\n",
            "Epoch 3/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9284 - accuracy: 0.7377\n",
            "Epoch 3: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9283 - accuracy: 0.7375 - val_loss: 0.9311 - val_accuracy: 0.7311\n",
            "Epoch 4/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9279 - accuracy: 0.7351\n",
            "Epoch 4: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9277 - accuracy: 0.7354 - val_loss: 0.9275 - val_accuracy: 0.7327\n",
            "Epoch 5/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9246 - accuracy: 0.7384\n",
            "Epoch 5: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9239 - accuracy: 0.7390 - val_loss: 0.9303 - val_accuracy: 0.7295\n",
            "Epoch 6/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9240 - accuracy: 0.7375\n",
            "Epoch 6: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9235 - accuracy: 0.7377 - val_loss: 0.9307 - val_accuracy: 0.7315\n",
            "Epoch 7/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9252 - accuracy: 0.7351\n",
            "Epoch 7: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9256 - accuracy: 0.7351 - val_loss: 0.9285 - val_accuracy: 0.7323\n",
            "Epoch 8/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9230 - accuracy: 0.7368\n",
            "Epoch 8: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9229 - accuracy: 0.7369 - val_loss: 0.9299 - val_accuracy: 0.7299\n",
            "Epoch 9/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9231 - accuracy: 0.7382\n",
            "Epoch 9: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9231 - accuracy: 0.7384 - val_loss: 0.9304 - val_accuracy: 0.7287\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9220 - accuracy: 0.7391\n",
            "Epoch 10: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9221 - accuracy: 0.7390 - val_loss: 0.9287 - val_accuracy: 0.7295\n",
            "Epoch 11/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9236 - accuracy: 0.7393\n",
            "Epoch 11: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9235 - accuracy: 0.7393 - val_loss: 0.9257 - val_accuracy: 0.7307\n",
            "Epoch 12/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9260 - accuracy: 0.7367\n",
            "Epoch 12: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9262 - accuracy: 0.7367 - val_loss: 0.9280 - val_accuracy: 0.7307\n",
            "Epoch 13/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9214 - accuracy: 0.7408\n",
            "Epoch 13: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9210 - accuracy: 0.7410 - val_loss: 0.9262 - val_accuracy: 0.7323\n",
            "Epoch 14/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9223 - accuracy: 0.7411\n",
            "Epoch 14: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9224 - accuracy: 0.7411 - val_loss: 0.9280 - val_accuracy: 0.7335\n",
            "Epoch 15/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9246 - accuracy: 0.7355\n",
            "Epoch 15: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9242 - accuracy: 0.7359 - val_loss: 0.9285 - val_accuracy: 0.7323\n",
            "Epoch 16/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9247 - accuracy: 0.7390\n",
            "Epoch 16: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9244 - accuracy: 0.7394 - val_loss: 0.9294 - val_accuracy: 0.7307\n",
            "Epoch 17/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9249 - accuracy: 0.7391\n",
            "Epoch 17: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9249 - accuracy: 0.7390 - val_loss: 0.9274 - val_accuracy: 0.7335\n",
            "Epoch 18/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9222 - accuracy: 0.7390\n",
            "Epoch 18: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9224 - accuracy: 0.7387 - val_loss: 0.9282 - val_accuracy: 0.7307\n",
            "Epoch 19/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9266 - accuracy: 0.7349\n",
            "Epoch 19: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9268 - accuracy: 0.7346 - val_loss: 0.9268 - val_accuracy: 0.7323\n",
            "Epoch 20/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9242 - accuracy: 0.7400\n",
            "Epoch 20: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9241 - accuracy: 0.7399 - val_loss: 0.9286 - val_accuracy: 0.7287\n",
            "Epoch 21/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9204 - accuracy: 0.7408\n",
            "Epoch 21: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9205 - accuracy: 0.7407 - val_loss: 0.9254 - val_accuracy: 0.7347\n",
            "Epoch 22/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9196 - accuracy: 0.7425\n",
            "Epoch 22: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9197 - accuracy: 0.7422 - val_loss: 0.9287 - val_accuracy: 0.7347\n",
            "Epoch 23/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9243 - accuracy: 0.7391\n",
            "Epoch 23: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9240 - accuracy: 0.7390 - val_loss: 0.9276 - val_accuracy: 0.7339\n",
            "Epoch 24/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9260 - accuracy: 0.7360\n",
            "Epoch 24: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9260 - accuracy: 0.7360 - val_loss: 0.9284 - val_accuracy: 0.7331\n",
            "Epoch 25/50\n",
            "171/176 [============================>.] - ETA: 0s - loss: 0.9211 - accuracy: 0.7419\n",
            "Epoch 25: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9213 - accuracy: 0.7416 - val_loss: 0.9269 - val_accuracy: 0.7319\n",
            "Epoch 26/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9227 - accuracy: 0.7390\n",
            "Epoch 26: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9224 - accuracy: 0.7392 - val_loss: 0.9268 - val_accuracy: 0.7319\n",
            "Epoch 27/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9254 - accuracy: 0.7362\n",
            "Epoch 27: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9256 - accuracy: 0.7362 - val_loss: 0.9271 - val_accuracy: 0.7311\n",
            "Epoch 28/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9247 - accuracy: 0.7374\n",
            "Epoch 28: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9244 - accuracy: 0.7378 - val_loss: 0.9278 - val_accuracy: 0.7295\n",
            "Epoch 29/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9237 - accuracy: 0.7364\n",
            "Epoch 29: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9232 - accuracy: 0.7368 - val_loss: 0.9263 - val_accuracy: 0.7319\n",
            "Epoch 30/50\n",
            "170/176 [===========================>..] - ETA: 0s - loss: 0.9244 - accuracy: 0.7401\n",
            "Epoch 30: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9249 - accuracy: 0.7395 - val_loss: 0.9270 - val_accuracy: 0.7335\n",
            "Epoch 31/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9225 - accuracy: 0.7373\n",
            "Epoch 31: saving model to magnitude_pruning_itr_13.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9223 - accuracy: 0.7374 - val_loss: 0.9277 - val_accuracy: 0.7331\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9277 - accuracy: 0.7331\n",
            "Post retraining val loss: 0.9276966452598572 | val acc: 0.7330693006515503 | sparsity: 0.9704367947137367\n",
            "Epoch 1/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9318 - accuracy: 0.7348\n",
            "Epoch 1: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 4s 11ms/step - loss: 0.9317 - accuracy: 0.7351 - val_loss: 0.9313 - val_accuracy: 0.7271\n",
            "Epoch 2/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9264 - accuracy: 0.7378\n",
            "Epoch 2: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9263 - accuracy: 0.7377 - val_loss: 0.9248 - val_accuracy: 0.7370\n",
            "Epoch 3/50\n",
            "173/176 [============================>.] - ETA: 0s - loss: 0.9270 - accuracy: 0.7357\n",
            "Epoch 3: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9275 - accuracy: 0.7353 - val_loss: 0.9274 - val_accuracy: 0.7319\n",
            "Epoch 4/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9237 - accuracy: 0.7361\n",
            "Epoch 4: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9236 - accuracy: 0.7359 - val_loss: 0.9273 - val_accuracy: 0.7343\n",
            "Epoch 5/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9238 - accuracy: 0.7409\n",
            "Epoch 5: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9240 - accuracy: 0.7407 - val_loss: 0.9279 - val_accuracy: 0.7303\n",
            "Epoch 6/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9229 - accuracy: 0.7364\n",
            "Epoch 6: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 10ms/step - loss: 0.9229 - accuracy: 0.7364 - val_loss: 0.9281 - val_accuracy: 0.7315\n",
            "Epoch 7/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9246 - accuracy: 0.7370\n",
            "Epoch 7: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9250 - accuracy: 0.7368 - val_loss: 0.9275 - val_accuracy: 0.7323\n",
            "Epoch 8/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9224 - accuracy: 0.7388\n",
            "Epoch 8: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9226 - accuracy: 0.7386 - val_loss: 0.9286 - val_accuracy: 0.7307\n",
            "Epoch 9/50\n",
            "174/176 [============================>.] - ETA: 0s - loss: 0.9243 - accuracy: 0.7409\n",
            "Epoch 9: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9245 - accuracy: 0.7410 - val_loss: 0.9259 - val_accuracy: 0.7327\n",
            "Epoch 10/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9263 - accuracy: 0.7366\n",
            "Epoch 10: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9265 - accuracy: 0.7365 - val_loss: 0.9280 - val_accuracy: 0.7323\n",
            "Epoch 11/50\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.9208 - accuracy: 0.7398\n",
            "Epoch 11: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9208 - accuracy: 0.7398 - val_loss: 0.9292 - val_accuracy: 0.7275\n",
            "Epoch 12/50\n",
            "175/176 [============================>.] - ETA: 0s - loss: 0.9217 - accuracy: 0.7388\n",
            "Epoch 12: saving model to magnitude_pruning_itr_14.h5\n",
            "176/176 [==============================] - 2s 9ms/step - loss: 0.9220 - accuracy: 0.7387 - val_loss: 0.9271 - val_accuracy: 0.7315\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.9271 - accuracy: 0.7315\n",
            "Post retraining val loss: 0.9270807504653931 | val acc: 0.73148512840271 | sparsity: 0.9709461271340944\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9c1222d4f488>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     custom_model.fit(\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_variable_creation_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m    \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         ._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m    765\u001b[0m             *args, **kwds))\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;34m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m       \u001b[0mconcrete_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_concrete_function_internal_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplaceholder_bound_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           concrete_function = self._create_concrete_function(\n\u001b[0m\u001b[1;32m    397\u001b[0m               args, kwargs, func_graph)\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     concrete_function = monomorphic_function.ConcreteFunction(\n\u001b[0;32m--> 300\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m           \u001b[0;31m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             return autograph.converted_call(\n\u001b[0m\u001b[1;32m   1190\u001b[0m                 \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[1;32m   1267\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             outputs = reduce_per_replica(\n\u001b[1;32m   1270\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1314\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1315\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1316\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3694\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3696\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m                 \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9c0ce4b80bcc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_gradients_aggregation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_internal_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m         return tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply_gradients_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribution_strategy_context.get_replica_context().merge_call(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m             distribution.extended.update(\n\u001b[0m\u001b[1;32m   1251\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2639\u001b[0;31m       return self._replica_ctx_update(\n\u001b[0m\u001b[1;32m   2640\u001b[0m           var, fn, args=args, kwargs=kwargs, group=group)\n\u001b[1;32m   2641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_replica_ctx_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2516\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2518\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreplica_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2520\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_gather_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3108\u001b[0m     merge_fn = autograph.tf_convert(\n\u001b[1;32m   3109\u001b[0m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 3110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3115\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   3116\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3117\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3118\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3119\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_fn\u001b[0;34m(_, *merged_args, **merged_kwargs)\u001b[0m\n\u001b[1;32m   2514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2515\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmerged_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmerged_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2516\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreplica_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2635\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m   2636\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2638\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2639\u001b[0m       return self._replica_ctx_update(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3708\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3709\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3710\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3712\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3714\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3715\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3716\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3717\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3718\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1245\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step_xla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1246\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[0;32m--> 142\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m    143\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplaceholder_bound_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           concrete_function = self._create_concrete_function(\n\u001b[0m\u001b[1;32m    397\u001b[0m               args, kwargs, func_graph)\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     concrete_function = monomorphic_function.ConcreteFunction(\n\u001b[0;32m--> 300\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m           \u001b[0;31m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             return autograph.converted_call(\n\u001b[0m\u001b[1;32m   1190\u001b[0m                 \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36mtf___update_step_xla\u001b[0;34m(self, gradient, variable, key)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;34mf\"`tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             )\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/adam.py\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mbeta_2_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mlocal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mbeta_1_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mbeta_2_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         )\n\u001b[1;32m   1017\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[0mTruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m   \u001b[0mTruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Truncate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   2014\u001b[0m         \"Cast\", x=x, DstT=DstT, Truncate=Truncate, name=name)\n\u001b[1;32m   2015\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    752\u001b[0m   \u001b[0;34m\"\"\"Implementation of apply_op that returns output_structure, op.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m   \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproducer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_GetOpDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mop_type_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_GetOpDef\u001b[0;34m(op_type_name, keywords)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0mproducer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def_versions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   6465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6466\u001b[0m   \"\"\"\n\u001b[0;32m-> 6467\u001b[0;31m   \u001b[0mcurrent_default_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6468\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcurrent_default_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilding_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6469\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_default_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "max_iterations = 100\n",
        "\n",
        "model = get_original_model()\n",
        "model_path = \"unstructured_pruning_v1_factor_1.6973684210526316_sparsity_0.9168725640165077_val_acc_0.21465346217155457.h5\"\n",
        "model_path = os.path.join(root_dir, model_path)\n",
        "model.load_weights(model_path)\n",
        "\n",
        "custom_model = get_custom_model(model.trainable_weights)\n",
        "custom_model.load_weights(model_path)\n",
        "\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=10, \n",
        "    monitor=\"val_accuracy\"\n",
        ")\n",
        "\n",
        "prev_acc = 0\n",
        "es_stop = 10\n",
        "\n",
        "val_accuracies = []\n",
        "sparsities = []\n",
        "\n",
        "for i in range(max_iterations):\n",
        "    if i > 0:\n",
        "        curr_weights_std = calc_std(custom_model.trainable_weights)\n",
        "        pruned_weights = prune(custom_model.trainable_weights, curr_weights_std, factor=3)\n",
        "        custom_model = get_custom_model(pruned_weights)\n",
        "        custom_model.set_weights(pruned_weights)\n",
        "\n",
        "    checkpoint_path = f\"magnitude_pruning_itr_{i}.h5\"\n",
        "\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "        monitor=\"val_accuracy\"\n",
        "    )\n",
        "\n",
        "    custom_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.0001, weight_decay=1e-6),\n",
        "        loss=categorical_loss_with_label_smoothing,\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    custom_model.fit(\n",
        "        x=train_images, \n",
        "        y=train_labels, \n",
        "        epochs=50, \n",
        "        batch_size=128, \n",
        "        validation_data=(val_images, val_labels), \n",
        "        callbacks=[cp_callback, es_callback]\n",
        "    )\n",
        "\n",
        "    post_results = custom_model.evaluate(val_images, val_labels)\n",
        "    model_sparsity = measure_sparsity(custom_model.trainable_weights)\n",
        "    \n",
        "    sparsities.append(model_sparsity)\n",
        "    val_accuracies.append(post_results[1])\n",
        "\n",
        "    print(f\"Post retraining val loss: {post_results[0]} | val acc: {post_results[1]} | sparsity: {model_sparsity}\")\n",
        "    \n",
        "    if post_results[1] < prev_acc:\n",
        "        es_stop -= 1\n",
        "    \n",
        "    if es_stop == 0:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0UhbURCy8SS",
        "outputId": "5e9a2fb8-c9c2-459a-af0a-32fe23dca3a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 0s 3ms/step - loss: 0.7593 - accuracy: 0.7347\n",
            "Post retraining val loss: 0.7592514753341675 | val acc: 0.7346534729003906 | sparsity: 0.9666168015610532\n"
          ]
        }
      ],
      "source": [
        "model = get_original_model()\n",
        "model_path = \"magnitude_pruning_itr_6.h5\"\n",
        "model.load_weights(model_path)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001, weight_decay=1e-6),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "final_results = model.evaluate(val_images, val_labels)\n",
        "final_model_sparsity = measure_sparsity(model.trainable_weights)\n",
        "\n",
        "print(f\"Post retraining val loss: {final_results[0]} | val acc: {final_results[1]} | sparsity: {final_model_sparsity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhEaV3iyMJGu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "prune",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
